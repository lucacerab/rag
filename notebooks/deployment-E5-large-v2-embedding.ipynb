{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a137ada-6d77-4873-8810-fc49a583d1e9",
   "metadata": {},
   "source": [
    "# Deploy [Microsoft E5 Large V2](https://huggingface.co/intfloat/e5-large-v2) on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba46be",
   "metadata": {},
   "source": [
    "This notebook provides a detailed walkthrough for deploying Microsoft E5 Large V2 model from HuggingFace on Amazon SageMaker. This model serves as the embedding model within a custom RAG architecture. For additional insights and in-depth information, please refer to the accompanying blog post.\n",
    "\n",
    "Steps:\n",
    "1. **Prepare the Deployment Package**\n",
    "    * Organize the necessary files including requirements.txt, serving.properties, and model.py within a designated directory.\n",
    "    * Package the directory contents into a tar.gz file.\n",
    "    * Upload the Deployment Package to Amazon S3\n",
    "\n",
    "2. **Upload the packaged tar.gz file to an Amazon S3 bucket**\n",
    "   - Upload the packaged `tar.gz` file to an Amazon S3 bucket. This serves as the storage location for the deployment package.\n",
    "\n",
    "3. **Deploy the Model as a SageMaker Endpoint**\n",
    "    - Utilize SageMaker's capabilities to deploy the packaged model as an endpoint for later API inference.\n",
    "\n",
    "*Note: This notebook assumes familiarity with Amazon SageMaker and basic concepts of deploying machine learning models. Additional documentation and resources are available for further reference and exploration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b44151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc3851a2",
   "metadata": {},
   "source": [
    "### 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1481e94-a7ba-4177-8fff-ca0e55888774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00befdf1-d382-49b6-b91a-5c1f0acc950e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "554c768b-9151-47e9-9a36-94f83beaf66b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f6c5a0-ffb8-4490-a760-c7a51b184d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "model = AutoModel.from_pretrained('intfloat/e5-large-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4cd74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f115a3a",
   "metadata": {},
   "source": [
    "### 1. Preparing deployment package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c773713f-2c4f-40fa-b353-f7b2e9d5e042",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'pt_save_pretrained'\n",
    "%mkdir {save_dir}\n",
    "%mkdir {save_dir}/code\n",
    "\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3ff7216-0cbb-4af0-9ebf-c020aae2a8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pt_save_pretrained/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {save_dir}/code/inference.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "  model = AutoModel.from_pretrained(model_dir)\n",
    "  return model, tokenizer\n",
    "\n",
    "def predict_fn(data, model_and_tokenizer):\n",
    "    model, tokenizer = model_and_tokenizer\n",
    "\n",
    "    input_texts = data.pop(\"inputs\")\n",
    "    batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    return {\"vectors\": embeddings.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1451906-8245-4f6c-b698-90a3df27de81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c46dd53f",
   "metadata": {},
   "source": [
    "### 2. Upload model artifacts gz file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae3c67f6-9561-4008-ab20-18605cd15ef5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TarFile.close of <tarfile.TarFile object at 0x7f47ac870190>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tar\n",
    "import tarfile\n",
    "model_s3_name = 'e5-large-v2-embedding-model.tar.gz'\n",
    "\n",
    "with tarfile.open(model_s3_name, 'w:gz') as f:\n",
    "    f.add('pt_save_pretrained/', arcname='.')\n",
    "    \n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed7fe225-2f65-4bb5-b82a-c56510cae839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to s3\n",
    "s3_bucket = 'ai-models'\n",
    "bucket_prefix = 'embeddings'\n",
    "\n",
    "model_filename = model_s3_name\n",
    "model_s3_key = f'{bucket_prefix}/' + model_filename\n",
    "model_url = f's3://{s3_bucket}/{model_s3_key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891142a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp e5-large-v2-embedding-model.tar.gz s3://{s3_bucket}/{model_s3_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f29922-d3cc-42ce-8937-109d7715f0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46421925-68ea-4672-ad36-7e3441e6bd50",
   "metadata": {},
   "source": [
    "### 3. Deploy as SageMaker Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14cb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "unix_time = int(time.time())\n",
    "endpoint_name = f\"{'e5-large-v2'}-{unix_time}\"\n",
    "print(f'Endpoint name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3cdd9b97-8c4e-4bb0-8819-844ea6ef3585",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=model_url,       # path to your model and script\n",
    "   role=role,                    # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.26\",  # transformers version used\n",
    "   pytorch_version=\"1.13\",        # pytorch version used\n",
    "   py_version='py39',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "predictor = huggingface_model.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.xlarge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa755e2-c6ca-45f7-8633-22717b23bd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461de79d-7d3e-440d-ba4b-cde2ab4d556b",
   "metadata": {},
   "source": [
    "### 4. Clean-up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c146104b-caa4-452e-86bb-5ca69256e362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#predictor.delete_model()\n",
    "#predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
