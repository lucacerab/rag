{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11061f1e",
   "metadata": {},
   "source": [
    "# Deploy Mistral-7B fine tuned model via DJL on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36e4b1f",
   "metadata": {},
   "source": [
    "This notebook serves as a comprehensive guide for deploying Mistral 7B Instruct - LoRA fine-tuned on Amazon SageMaker using DeepSpeed and DJL. Refer to this [AWS Blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/) for more details. This model served as fine-tuned head of a custom RAG architecture, for more details check the blog post.\n",
    "\n",
    "Steps:\n",
    "1. **Prepare the Deployment Package**\n",
    "    * Organize the necessary files including requirements.txt, serving.properties, and model.py within a designated directory.\n",
    "    * Package the directory contents into a tar.gz file.\n",
    "    * Upload the Deployment Package to Amazon S3\n",
    "\n",
    "2. **Upload the packaged tar.gz file to an Amazon S3 bucket**\n",
    "   - Upload the packaged `tar.gz` file to an Amazon S3 bucket. This serves as the storage location for the deployment package.\n",
    "\n",
    "3. **Deploy the Model as a SageMaker Endpoint**\n",
    "    - Utilize SageMaker's capabilities to deploy the packaged model as an endpoint for later API inference.\n",
    "\n",
    "*Note: This notebook assumes familiarity with Amazon SageMaker, DJL, and basic concepts of deploying machine learning models. Additional documentation and resources are available for further reference and exploration.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ebbab3",
   "metadata": {},
   "source": [
    "### 0. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93338149",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c244487f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "from sagemaker import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ee030be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name\n",
    "\n",
    "image_uri = image_uris.retrieve(framework=\"djl-deepspeed\", version=\"0.24.0\", region=session._region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f356c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "788c5249",
   "metadata": {},
   "source": [
    "### 1. Preparing deployment package\n",
    "Our directory should have the following structure:\n",
    "\n",
    "faber_lora\n",
    "├── model.py\n",
    "├── serving.properties\n",
    "├── requirements.txt\n",
    "└── fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ced44b2-3222-4193-999e-a123c5c8c837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p faber_lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8234b",
   "metadata": {},
   "source": [
    "Prepare requirements.txt and serving.properties in ./faber_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45413ead-eba0-4f9d-8fc7-c765168589d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing faber_lora/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile faber_lora/serving.properties\n",
    "engine=Python\n",
    "option.model_id=mistralai/Mistral-7B-Instruct-v0.1\n",
    "option.dtype=fp16\n",
    "option.tensor_parallel_degree=4\n",
    "option.enable_streaming=true\n",
    "option.entryPoint=model.py\n",
    "option.adapter_checkpoint=mistral-ft-doc-285-gess\n",
    "option.adapter_name=mistral-lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9df2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing faber_lora/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile faber_lora/requirements.txt\n",
    "git+https://github.com/huggingface/transformers\n",
    "accelerate==0.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a53d6a",
   "metadata": {},
   "source": [
    "Prepare model.py in ./faber_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c834d798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing faber_lora/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile faber_lora/model.py\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from djl_python.inputs import Input\n",
    "from djl_python.outputs import Output\n",
    "from djl_python.encode_decode import encode, decode\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from transformers import Pipeline, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "# Create Instruct Pipeline\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "END_KEY = \"### End\"\n",
    "INTRO_BLURB = (\n",
    "    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    ")\n",
    "\n",
    "\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "\n",
    "def get_special_token_id(tokenizer: PreTrainedTokenizer, key: str) -> int:\n",
    "    token_ids = tokenizer.encode(key)\n",
    "    if len(token_ids) > 1:\n",
    "        raise ValueError(f\"Expected only a single token for '{key}' but found {token_ids}\")\n",
    "    return token_ids[0]\n",
    "\n",
    "\n",
    "class InstructionTextGenerationPipeline(Pipeline):\n",
    "    def __init__(\n",
    "        self, *args, do_sample: bool = True, max_new_tokens: int = 512, temperature: float = 0.3, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, do_sample=do_sample, max_new_tokens=max_new_tokens, temperature=temperature, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, return_instruction_text=False, **generate_kwargs):\n",
    "        preprocess_params = {}\n",
    "\n",
    "        tokenizer_response_key = next(\n",
    "            (token for token in self.tokenizer.additional_special_tokens if token.startswith(RESPONSE_KEY)), None\n",
    "        )\n",
    "\n",
    "        response_key_token_id = None\n",
    "        end_key_token_id = None\n",
    "        if tokenizer_response_key:\n",
    "            try:\n",
    "                response_key_token_id = get_special_token_id(self.tokenizer, tokenizer_response_key)\n",
    "                end_key_token_id = get_special_token_id(self.tokenizer, END_KEY)\n",
    "\n",
    "                generate_kwargs[\"eos_token_id\"] = end_key_token_id\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        forward_params = generate_kwargs\n",
    "        postprocess_params = {\n",
    "            \"response_key_token_id\": response_key_token_id,\n",
    "            \"end_key_token_id\": end_key_token_id,\n",
    "            \"return_instruction_text\": return_instruction_text,\n",
    "        }\n",
    "\n",
    "        return preprocess_params, forward_params, postprocess_params\n",
    "\n",
    "    def preprocess(self, instruction_text, **generate_kwargs):\n",
    "        prompt_text = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction_text)\n",
    "        inputs = self.tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs[\"prompt_text\"] = prompt_text\n",
    "        inputs[\"instruction_text\"] = instruction_text\n",
    "        return inputs\n",
    "\n",
    "    def _forward(self, model_inputs, **generate_kwargs):\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
    "        generated_sequence = self.model.generate(\n",
    "            input_ids=input_ids.to(self.model.device),\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            **generate_kwargs,\n",
    "        )[0] \n",
    "        instruction_text = model_inputs.pop(\"instruction_text\")\n",
    "        return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"instruction_text\": instruction_text}\n",
    "\n",
    "    def postprocess(self, model_outputs, response_key_token_id, end_key_token_id, return_instruction_text):\n",
    "        sequence = model_outputs[\"generated_sequence\"]\n",
    "        instruction_text = model_outputs[\"instruction_text\"]\n",
    "\n",
    "        decoded = None\n",
    "\n",
    "        if response_key_token_id and end_key_token_id:\n",
    "            response_pos = None\n",
    "            response_positions = np.where(sequence == response_key_token_id)[0]\n",
    "            if len(response_positions) == 0:\n",
    "                logger.warn(f\"Could not find response key {response_key_token_id} in: {sequence}\")\n",
    "            else:\n",
    "                response_pos = response_positions[0]\n",
    "\n",
    "            if response_pos:\n",
    "                end_pos = None\n",
    "                end_positions = np.where(sequence == end_key_token_id)[0]\n",
    "                if len(end_positions) > 0:\n",
    "                    end_pos = end_positions[0]\n",
    "\n",
    "                decoded = self.tokenizer.decode(sequence[response_pos + 1 : end_pos]).strip()\n",
    "        else:\n",
    "\n",
    "            fully_decoded = self.tokenizer.decode(sequence)\n",
    "\n",
    "            m = re.search(r\"#+\\s*Response:\\s*(.+?)#+\\s*End\", fully_decoded, flags=re.DOTALL)\n",
    "\n",
    "            if m:\n",
    "                decoded = m.group(1).strip()\n",
    "            else:\n",
    "                m = re.search(r\"#+\\s*Response:\\s*(.+)\", fully_decoded, flags=re.DOTALL)\n",
    "                if m:\n",
    "                    decoded = m.group(1).strip()\n",
    "                else:\n",
    "                    logger.warn(f\"Failed to find response in:\\n{fully_decoded}\")\n",
    "        \n",
    "        if (\"### \" in decoded):\n",
    "            decoded = decoded.split(\"### \")[0]\n",
    "        \n",
    "        if (\"[STOP][STOP]\"  in decoded):\n",
    "            decoded = decoded.split(\"[STOP][STOP]\")[0]\n",
    "        \n",
    "        if return_instruction_text:\n",
    "            return {\"instruction_text\": instruction_text, \"generated_text\": decoded}\n",
    "\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "                   ### Instruction: {instruction}\n",
    "                   ### Input: {input}\n",
    "                   ### Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "                   ### Instruction:\n",
    "                   {instruction}\n",
    "                   ### Response:\"\"\"\n",
    "\n",
    "def evaluate(instruction,\n",
    "        input=None,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=256,\n",
    "        **kwargs,\n",
    "):\n",
    "    model_gen = InstructionTextGenerationPipeline(model=model, tokenizer=tokenizer)    \n",
    "    response = model_gen(instruction)    \n",
    "    return response\n",
    " \n",
    "    \n",
    "def load_base_model(adapter_checkpoint, adapter_name):\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(model, adapter_checkpoint, adapter_name)    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def inference(inputs: Input):\n",
    "    json_input = decode(inputs, \"application/json\")\n",
    "    sequence = json_input.get(\"inputs\")\n",
    "    generation_kwargs = json_input.get(\"parameters\", {})\n",
    "    output = Output()\n",
    "    outs = evaluate(sequence)\n",
    "    encode(output, outs, \"application/json\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def handle(inputs: Input):\n",
    "    \"\"\"\n",
    "    Default handler function\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    if not model:\n",
    "        # stateful model\n",
    "        props = inputs.get_properties()\n",
    "        model, tokenizer = load_base_model(props.get(\"adapter_checkpoint\"), props.get(\"adapter_name\"))\n",
    "\n",
    "    if inputs.is_empty():\n",
    "        # initialization request\n",
    "        return None\n",
    "\n",
    "    return inference(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f69307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff85cf7e",
   "metadata": {},
   "source": [
    "### 2. Upload model artifacts gz file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a42a124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp -r models/mistral-ft-doc-285-gess faber_lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371b375e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faber_lora/\n",
      "faber_lora/requirements.txt\n",
      "faber_lora/mistral-ft-doc-285-gess/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-58-34_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-58-34_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698382714.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-03-26_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-03-26_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698379409.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_02-09-04_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_02-09-04_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698372547.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_03-33-29_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_03-33-29_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698377616.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-30-29_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-30-29_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698381042.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_06-05-23_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_06-05-23_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698386731.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_02-07-59_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_02-07-59_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698372482.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_02-07-15_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_02-07-15_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698372439.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-57-26_ip-172-16-10-151.us-west-2.compute.internal/\n",
      "faber_lora/mistral-ft-doc-285-gess/runs/Oct27_04-57-26_ip-172-16-10-151.us-west-2.compute.internal/events.out.tfevents.1698382649.ip-172-16-10-151.us-west-2.compute.internal\n",
      "faber_lora/mistral-ft-doc-285-gess/adapter_config.json\n",
      "faber_lora/mistral-ft-doc-285-gess/added_tokens.json\n",
      "faber_lora/mistral-ft-doc-285-gess/tokenizer_config.json\n",
      "faber_lora/mistral-ft-doc-285-gess/tokenizer.model\n",
      "faber_lora/mistral-ft-doc-285-gess/training_args.bin\n",
      "faber_lora/mistral-ft-doc-285-gess/adapter_model.bin\n",
      "faber_lora/mistral-ft-doc-285-gess/special_tokens_map.json\n",
      "faber_lora/mistral-ft-doc-285-gess/tokenizer.json\n",
      "faber_lora/mistral-ft-doc-285-gess/README.md\n",
      "faber_lora/mistral-ft-doc-285-gess/.ipynb_checkpoints/\n",
      "faber_lora/mistral-ft-doc-285-gess/.ipynb_checkpoints/tokenizer-checkpoint.json\n",
      "faber_lora/mistral-ft-doc-285-gess/.ipynb_checkpoints/adapter_config-checkpoint.json\n",
      "faber_lora/serving.properties\n",
      "faber_lora/model.py\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tar -cvzf faber_model.tar.gz faber_lora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 cp faber_model.tar.gz s3://ai-models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75689d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e900557",
   "metadata": {},
   "source": [
    "### 3. Deploy as SageMaker Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3be9709f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#instance_type = \"ml.p3.8xlarge\"     # 64GB GPU Memory\n",
    "#instance_type = \"ml.p2.8xlarge\"      # 96GB GPU Memory\n",
    "instance_type = \"ml.g5.12xlarge\"      # 96GB GPU Memory\n",
    "#instance_type = \"ml.g5.2xlarge\"  \n",
    "\n",
    "model_s3_location = \"s3://ai-models/faber_model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f90fb71a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.djl_inference\n",
    "\n",
    "model = Model(\n",
    "    image_uri,\n",
    "    model_data=model_s3_location,\n",
    "    predictor_cls = sagemaker.djl_inference.DJLPredictor, \n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0cf3606",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a9102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf21567-3baa-4486-9f6a-fbafdf6d79ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Testing the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca22abfd-f8b1-4768-92b8-627224923d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "endpoint = 'djl-inference-2024-04-11-10-43-22-559'\n",
    "runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"hey, how are you doing?\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.3,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5313e0c-9f7b-417f-92a3-82de0396c14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(EndpointName=endpoint,\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=json.dumps(payload).encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e78719",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = json.loads(response['Body'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52c74ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer program, so I don't have feelings or the ability to respond in a personal way.  However, I'm here to help you with any questions you have about the technical requirements for the building services design. Is there something specific you would like to know?</s>\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411bfb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9693f76f",
   "metadata": {},
   "source": [
    "### 5. Clean-up resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b462c95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()\n",
    "# model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623440a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
